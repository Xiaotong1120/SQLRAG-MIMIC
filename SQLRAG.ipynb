{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API configuration\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# PostgreSQL database connection\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5433\"  # Ensure this matches your PostgreSQL container port\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"Create and return database connection and cursor\"\"\"\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    cur = conn.cursor()\n",
    "    return conn, cur\n",
    "\n",
    "def fetch_all_metadata():\n",
    "    \"\"\"Fetch metadata for all tables\"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        # Get metadata for all tables\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_name, description, table_purpose, columns_info, \n",
    "                   primary_keys, foreign_keys, important_considerations,\n",
    "                   common_joins, example_questions\n",
    "            FROM mimic_table_metadata;\n",
    "        \"\"\")\n",
    "        \n",
    "        all_metadata = cur.fetchall()\n",
    "        \n",
    "        # Format metadata as dictionary\n",
    "        tables_metadata = {}\n",
    "        for row in all_metadata:\n",
    "            table_name = row[0]\n",
    "            tables_metadata[table_name] = {\n",
    "                'description': row[1],\n",
    "                'table_purpose': row[2],\n",
    "                'columns_info': row[3],\n",
    "                'primary_keys': row[4],\n",
    "                'foreign_keys': row[5],\n",
    "                'important_considerations': row[6],\n",
    "                'common_joins': row[7],\n",
    "                'example_questions': row[8]\n",
    "            }\n",
    "        \n",
    "        return tables_metadata\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format metadata into text suitable for LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata_for_prompt(metadata):\n",
    "    \"\"\"Format metadata into text suitable for LLM prompt\"\"\"\n",
    "    formatted_text = \"# Database Schema Information\\n\\n\"\n",
    "    \n",
    "    for table_name, table_info in metadata.items():\n",
    "        formatted_text += f\"## Table: {table_name}\\n\"\n",
    "        formatted_text += f\"Description: {table_info['description']}\\n\"\n",
    "        formatted_text += f\"Purpose: {table_info['table_purpose']}\\n\\n\"\n",
    "        \n",
    "        # Add primary key information\n",
    "        if table_info['primary_keys']:\n",
    "            formatted_text += f\"Primary Keys: {', '.join(table_info['primary_keys'])}\\n\\n\"\n",
    "        \n",
    "        # Add foreign key information\n",
    "        if table_info['foreign_keys']:\n",
    "            formatted_text += \"Foreign Keys:\\n\"\n",
    "            for fk_col, fk_info in table_info['foreign_keys'].items():\n",
    "                formatted_text += f\"- {fk_col} references {fk_info['table']}.{fk_info['column']}\\n\"\n",
    "            formatted_text += \"\\n\"\n",
    "        \n",
    "        # Add column information\n",
    "        formatted_text += \"Columns:\\n\"\n",
    "        for col_name, col_info in table_info['columns_info'].items():\n",
    "            formatted_text += f\"- {col_name} ({col_info['data_type']}): {col_info['description']}\\n\"\n",
    "            \n",
    "            # Add categorical value distribution if available and not too long\n",
    "            if 'categorical_values' in col_info and len(col_info['categorical_values']) < 15:\n",
    "                formatted_text += f\"  Possible values: {', '.join(col_info['categorical_values'])}\\n\"\n",
    "            \n",
    "            # Add value range if available\n",
    "            if 'value_range' in col_info:\n",
    "                formatted_text += f\"  Range: {col_info['value_range']['min']} to {col_info['value_range']['max']}\\n\"\n",
    "        \n",
    "        formatted_text += \"\\n\"\n",
    "        \n",
    "        # Add important considerations\n",
    "        if table_info['important_considerations']:\n",
    "            formatted_text += f\"Important Considerations: {table_info['important_considerations']}\\n\\n\"\n",
    "        \n",
    "        # Add common joins\n",
    "        if table_info['common_joins']:\n",
    "            formatted_text += \"Common Joins:\\n\"\n",
    "            for join in table_info['common_joins']:\n",
    "                formatted_text += f\"- {join}\\n\"\n",
    "            formatted_text += \"\\n\"\n",
    "        \n",
    "        formatted_text += \"---\\n\\n\"\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch metadata for tables most relevant to the query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_relevant_metadata(query_embedding, top_k):\n",
    "    \"\"\"\n",
    "    Fetch metadata for tables most relevant to the query embedding\n",
    "    \n",
    "    Parameters:\n",
    "        query_embedding (list): Vector representation of the user query\n",
    "        top_k (int): Number of most relevant tables to return\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping table names to their metadata with similarity scores\n",
    "    \"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Computing similarity to find top {top_k} relevant tables\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                table_name, description, table_purpose, columns_info, \n",
    "                primary_keys, foreign_keys, important_considerations,\n",
    "                common_joins, example_questions, embedding\n",
    "            FROM mimic_table_metadata\n",
    "            WHERE embedding IS NOT NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        rows = cur.fetchall()\n",
    "        \n",
    "        # Calculate similarity for each table\n",
    "        table_similarities = []\n",
    "        for row in rows:\n",
    "            table_name = row[0]\n",
    "            table_embedding = row[9]\n",
    "            \n",
    "            # Skip if embedding is NULL\n",
    "            if table_embedding is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert string embedding to list of floats if needed\n",
    "            if isinstance(table_embedding, str):\n",
    "                import json\n",
    "                table_embedding = json.loads(table_embedding.replace(\"'\", '\"'))\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(query_embedding, table_embedding)\n",
    "            table_similarities.append((row, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending) and take top_k\n",
    "        table_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_tables = table_similarities[:top_k]\n",
    "        \n",
    "        # Format as dictionary\n",
    "        tables_metadata = {}\n",
    "        for row, similarity in top_tables:\n",
    "            table_name = row[0]\n",
    "            tables_metadata[table_name] = {\n",
    "                'description': row[1],\n",
    "                'table_purpose': row[2],\n",
    "                'columns_info': row[3],\n",
    "                'primary_keys': row[4],\n",
    "                'foreign_keys': row[5],\n",
    "                'important_considerations': row[6],\n",
    "                'common_joins': row[7],\n",
    "                'example_questions': row[8],\n",
    "                'similarity_score': similarity  # Add similarity score to metadata\n",
    "            }\n",
    "            \n",
    "            # Print similarity for debugging\n",
    "            print(f\"Table: {table_name}, Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        return tables_metadata\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute cosine similarity between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors\n",
    "    \n",
    "    Parameters:\n",
    "        vec1 (list): First vector\n",
    "        vec2 (list): Second vector\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity (between -1 and 1)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the prompt to send to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompt(user_question, metadata_text):\n",
    "    \"\"\"Create the complete prompt to send to the LLM with improved instructions\"\"\"\n",
    "    # Add explicit column information to the prompt\n",
    "    prompt = f\"\"\"You are a professional medical database expert specializing in SQL and the MIMIC-IV database. Based on the user's question and the provided database metadata, generate a PostgreSQL query.\n",
    "\n",
    "## User Question\n",
    "{user_question}\n",
    "\n",
    "## Database Metadata\n",
    "{metadata_text}\n",
    "\n",
    "## Task\n",
    "1. Analyze the user question to determine which tables and columns need to be queried\n",
    "2. Design an effective SQL query based on the provided metadata\n",
    "3. Ensure the generated SQL is syntactically correct and considers table relationships\n",
    "4. Use ONLY columns that are explicitly mentioned in the metadata for each table\n",
    "5. If multiple table joins are needed, use the correct join conditions\n",
    "6. Handle any potential edge cases\n",
    "7. When dealing with medical codes (ICD diagnosis/procedure codes, medication codes), always join with their respective descriptor tables (d_icd_diagnoses, d_icd_procedures) to include both codes AND their human-readable descriptions\n",
    "8. For medications, include the actual drug names from prescriptions.drug or emar.medication rather than just codes\n",
    "\n",
    "## Response Format\n",
    "Please return ONLY the SQL query without any explanation or comments. Start your answer with \"SELECT\" or \"WITH\" and end with a semicolon. Do not include anything else.\n",
    "\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SQL query using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_with_openai(prompt):\n",
    "    \"\"\"Generate SQL query using OpenAI API with improved cleaning and validation\"\"\"\n",
    "    try:\n",
    "        # Using the new client format\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # or another suitable model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical database expert who converts natural language questions into PostgreSQL queries. Return ONLY the SQL query with no explanations or comments.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,  # Low temperature for more deterministic output\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Get raw content\n",
    "        raw_content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # More comprehensive cleaning of markdown and prefixes\n",
    "        # Remove common prefixes\n",
    "        prefixes = [\"SQL Query:\", \"Query:\", \"PostgreSQL Query:\"]\n",
    "        for prefix in prefixes:\n",
    "            if raw_content.startswith(prefix):\n",
    "                raw_content = raw_content[len(prefix):].strip()\n",
    "        \n",
    "        # Remove markdown code blocks (handling various formats)\n",
    "        import re\n",
    "        sql_query = re.sub(r'```(?:sql|postgresql)?|```', '', raw_content)\n",
    "        sql_query = sql_query.strip()\n",
    "        \n",
    "        # Find the termination point of the SQL part - look for typical SQL statement ending (semicolon) followed by a newline\n",
    "        # This will remove explanatory text after the query\n",
    "        match = re.search(r';[\\s\\n]*(\\n|$)', sql_query)\n",
    "        if match:\n",
    "            # Only keep the part up to the semicolon\n",
    "            sql_query = sql_query[:match.end()].strip()\n",
    "        \n",
    "        # Basic SQL syntax validation\n",
    "        if not sql_query.lower().startswith(('select', 'with')):\n",
    "            print(\"Warning: Generated SQL may not be valid. It doesn't start with SELECT or WITH.\")\n",
    "            \n",
    "        # Log the cleaned query for debugging\n",
    "        print(f\"Cleaned SQL query: {sql_query[:100]}...\")\n",
    "        \n",
    "        return sql_query\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_table_structure(table_name):\n",
    "    \"\"\"Get the actual column structure of a table, returns a list of column names\"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        # Get column names for the table\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = '{table_name}'\n",
    "        \"\"\")\n",
    "        \n",
    "        columns = [row[0] for row in cur.fetchall()]\n",
    "        print(f\"Columns in table {table_name}: {', '.join(columns)}\")\n",
    "        return columns\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting structure for table {table_name}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def check_query_columns(sql_query):\n",
    "    \"\"\"Analyze SQL query, validate that all referenced tables and columns exist\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Extract tables used in the query\n",
    "    from_pattern = re.compile(r'\\bFROM\\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "    join_pattern = re.compile(r'\\bJOIN\\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "    \n",
    "    tables = from_pattern.findall(sql_query) + join_pattern.findall(sql_query)\n",
    "    tables = list(set(tables))  # Remove duplicates\n",
    "    \n",
    "    # Get the actual column structure for each table\n",
    "    table_columns = {}\n",
    "    for table in tables:\n",
    "        table_columns[table] = validate_table_structure(table)\n",
    "    \n",
    "    # A simple method to find column references in the query\n",
    "    # This is a simplified version; a complete SQL parser would be needed for full accuracy\n",
    "    potential_issues = []\n",
    "    \n",
    "    for table in tables:\n",
    "        columns = table_columns[table]\n",
    "        # Look for patterns like \"table.column\"\n",
    "        table_column_pattern = re.compile(rf'\\b{table}\\.([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "        referenced_columns = table_column_pattern.findall(sql_query)\n",
    "        \n",
    "        for col in referenced_columns:\n",
    "            if col not in columns:\n",
    "                potential_issues.append(f\"Warning: Column '{col}' does not exist in table '{table}'\")\n",
    "    \n",
    "    return potential_issues, table_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute SQL query and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_query(sql_query, timeout_seconds=3000):\n",
    "    \"\"\"Execute SQL query with timeout and improved error handling\"\"\"\n",
    "    conn = None\n",
    "    cur = None\n",
    "    \n",
    "    try:\n",
    "        # Get a fresh connection\n",
    "        conn = psycopg2.connect(**DB_PARAMS)\n",
    "        \n",
    "        # Enable autocommit for session parameter changes\n",
    "        conn.autocommit = True\n",
    "        \n",
    "        # Create cursor\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Set statement timeout before starting transaction\n",
    "        cur.execute(f\"SET statement_timeout = {timeout_seconds * 1000};\")  # milliseconds\n",
    "        \n",
    "        # Switch to transaction mode for the actual query\n",
    "        conn.autocommit = False\n",
    "        \n",
    "        # Log the query being executed\n",
    "        print(f\"Executing SQL (with {timeout_seconds}s timeout): {sql_query[:200]}...\")\n",
    "        \n",
    "        # Execute the query\n",
    "        cur.execute(sql_query)\n",
    "        \n",
    "        # Get column names if the query returns results\n",
    "        if cur.description:\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            \n",
    "            # Fetch results with a row limit to avoid memory issues\n",
    "            results = []\n",
    "            while True:\n",
    "                batch = cur.fetchmany(1000)  # Fetch in batches\n",
    "                if not batch:\n",
    "                    break\n",
    "                results.extend(batch)\n",
    "                \n",
    "                # Check if we've fetched enough rows\n",
    "                if len(results) >= 10000:  # Set a reasonable maximum\n",
    "                    print(\"Warning: Query returned more than 10,000 rows, truncating results\")\n",
    "                    break\n",
    "            \n",
    "            # Commit transaction\n",
    "            conn.commit()\n",
    "            \n",
    "            # Convert results to DataFrame\n",
    "            df = pd.DataFrame(results, columns=column_names)\n",
    "            \n",
    "            print(f\"Query returned {len(df)} rows and {len(df.columns)} columns\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            # For queries that don't return results (e.g., INSERT, UPDATE)\n",
    "            conn.commit()\n",
    "            print(\"Query executed successfully (no results returned)\")\n",
    "            return pd.DataFrame()  # Empty DataFrame\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        if conn:\n",
    "            try:\n",
    "                conn.rollback()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        error_msg = f\"Error executing SQL query: {e}\"\n",
    "        print(error_msg)\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        if cur:\n",
    "            try:\n",
    "                # Reset statement timeout if possible\n",
    "                if conn and conn.status == psycopg2.extensions.STATUS_READY:\n",
    "                    conn.autocommit = True\n",
    "                    cur.execute(\"RESET statement_timeout;\")\n",
    "            except:\n",
    "                pass\n",
    "            cur.close()\n",
    "        \n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a user's natural language query into a vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_user_query(query_text):\n",
    "    \"\"\"\n",
    "    Convert a user's natural language query into a vector representation\n",
    "    \n",
    "    Parameters:\n",
    "        query_text (str): The user's natural language query\n",
    "        \n",
    "    Returns:\n",
    "        list: Vector representation of the query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the query text - more processing steps can be added as needed\n",
    "        processed_query = query_text.strip()\n",
    "        \n",
    "        # Generate embedding vector using OpenAI API\n",
    "        response = client.embeddings.create(\n",
    "            input=processed_query,\n",
    "            model=\"text-embedding-3-small\"  # Use the same model as for table embeddings\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding vector\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        print(f\"✅ Successfully vectorized query: '{query_text[:50]}...' if len(query_text) > 50 else query_text\")\n",
    "        return query_embedding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error vectorizing query: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format a simplified version of metadata for the answer generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata_for_answer(metadata):\n",
    "    \"\"\"Format a simplified version of metadata for the answer generation prompt\"\"\"\n",
    "    formatted_text = \"\"\n",
    "    \n",
    "    for table_name, table_info in metadata.items():\n",
    "        formatted_text += f\"Table: {table_name}\\n\"\n",
    "        formatted_text += f\"Description: {table_info['description']}\\n\\n\"\n",
    "        \n",
    "        # Add key columns (simplified)\n",
    "        formatted_text += \"Key columns:\\n\"\n",
    "        for col_name, col_info in table_info['columns_info'].items():\n",
    "            if col_name in table_info.get('primary_keys', []) or 'key' in col_name.lower():\n",
    "                formatted_text += f\"- {col_name}: {col_info['description']}\\n\"\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the highest similarity score and corresponding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_similarity(metadata):\n",
    "    \"\"\"Extract the highest similarity score and corresponding table\"\"\"\n",
    "    best_table = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for table_name, info in metadata.items():\n",
    "        if \"similarity_score\" in info and info[\"similarity_score\"] > best_score:\n",
    "            best_score = info[\"similarity_score\"]\n",
    "            best_table = table_name\n",
    "    \n",
    "    return best_table, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlrag_pipeline(user_question):\n",
    "    \"\"\"Execute the complete SQLRAG pipeline with enhanced safety checks\"\"\"\n",
    "    print(f\"User Question: {user_question}\")\n",
    "    \n",
    "    # 1. Vectorize user query\n",
    "    print(\"Vectorizing user query...\")\n",
    "    query_embedding = vectorize_user_query(user_question)\n",
    "    \n",
    "    if not query_embedding:\n",
    "        return {\"error\": \"Failed to vectorize user query\"}\n",
    "    \n",
    "    # 2. Fetch relevant metadata using vector similarity\n",
    "    print(\"Finding relevant tables...\")\n",
    "    metadata = fetch_relevant_metadata(query_embedding, top_k=10)\n",
    "    \n",
    "    # 3. Check if any relevant tables were found with good similarity\n",
    "    if not metadata:\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": \"No relevant tables found in the database for this query.\",\n",
    "            \"answer\": \"I don't have the necessary data to answer this question. The database doesn't contain information related to your query.\"\n",
    "        }\n",
    "    \n",
    "    # 4. Check similarity scores to ensure they're above threshold\n",
    "    # Get the highest similarity score\n",
    "    best_table, best_similarity = get_highest_similarity(metadata)\n",
    "    if best_similarity < 0.2:  # Adjust threshold as needed\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"best_match\": best_table,\n",
    "            \"similarity\": best_similarity,\n",
    "            \"error\": \"The query doesn't seem to match well with available data.\",\n",
    "            \"answer\": f\"Your question might not be answerable with the available medical data. The closest match I found was related to '{best_table}' but the relevance is low.\"\n",
    "        }\n",
    "    \n",
    "    # 5. Use query classifier to identify query intent and feasibility\n",
    "    query_classification = classify_query(user_question, metadata)\n",
    "    if query_classification[\"status\"] == \"not_supported\":\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": query_classification[\"reason\"],\n",
    "            \"answer\": query_classification[\"message\"]\n",
    "        }\n",
    "    \n",
    "    # 6. Format metadata for prompt - ensure metadata includes complete column information\n",
    "    metadata_text = format_metadata_for_prompt(metadata)\n",
    "    \n",
    "    # 7. Create LLM prompt with improved instructions\n",
    "    prompt = create_llm_prompt(user_question, metadata_text)\n",
    "    \n",
    "    print(\"Generating SQL with LLM...\")\n",
    "    # 8. Generate SQL query\n",
    "    sql_query = generate_sql_with_openai(prompt)\n",
    "    \n",
    "    if not sql_query:\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": \"Failed to generate SQL query\",\n",
    "            \"answer\": \"I couldn't generate a SQL query to answer your question. Please try rephrasing it.\"\n",
    "        }\n",
    "    \n",
    "    print(f\"Generated SQL: \\n{sql_query}\\n\")\n",
    "    \n",
    "    # 9. New step: Validate the generated SQL against table structure\n",
    "    print(\"Validating SQL against database structure...\")\n",
    "    issues, table_columns = check_query_columns(sql_query)\n",
    "    \n",
    "    if issues:\n",
    "        print(\"Potential issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        \n",
    "        # Try to automatically fix the SQL\n",
    "        print(\"Attempting to fix SQL...\")\n",
    "        fixed_sql = attempt_fix_sql(sql_query, table_columns)\n",
    "        \n",
    "        if fixed_sql:\n",
    "            print(f\"Fixed SQL: \\n{fixed_sql}\\n\")\n",
    "            sql_query = fixed_sql\n",
    "        else:\n",
    "            return {\n",
    "                \"user_question\": user_question, \n",
    "                \"error\": f\"Generated SQL query is incompatible with database structure: {'; '.join(issues)}\",\n",
    "                \"generated_sql\": sql_query,\n",
    "                \"answer\": \"I couldn't generate a valid query compatible with the database structure. There might be a mismatch in my understanding of the database schema.\"\n",
    "            }\n",
    "    \n",
    "    print(\"Executing SQL query...\")\n",
    "    # 10. Execute SQL query\n",
    "    results = execute_sql_query(sql_query)\n",
    "    \n",
    "    if results is None:\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": \"Error executing SQL query\",\n",
    "            \"generated_sql\": sql_query,\n",
    "            \"answer\": \"I apologize, but I couldn't execute the generated SQL query. There might be an issue with the database or the query structure.\"\n",
    "        }\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
