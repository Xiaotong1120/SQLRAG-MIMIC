{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API configuration\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# PostgreSQL database connection\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5433\"  # Ensure this matches your PostgreSQL container port\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"Create and return database connection and cursor\"\"\"\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    cur = conn.cursor()\n",
    "    return conn, cur\n",
    "\n",
    "def fetch_all_metadata():\n",
    "    \"\"\"Fetch metadata for all tables\"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        # Get metadata for all tables\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_name, description, table_purpose, columns_info, \n",
    "                   primary_keys, foreign_keys, important_considerations,\n",
    "                   common_joins, example_questions\n",
    "            FROM mimic_table_metadata;\n",
    "        \"\"\")\n",
    "        \n",
    "        all_metadata = cur.fetchall()\n",
    "        \n",
    "        # Format metadata as dictionary\n",
    "        tables_metadata = {}\n",
    "        for row in all_metadata:\n",
    "            table_name = row[0]\n",
    "            tables_metadata[table_name] = {\n",
    "                'description': row[1],\n",
    "                'table_purpose': row[2],\n",
    "                'columns_info': row[3],\n",
    "                'primary_keys': row[4],\n",
    "                'foreign_keys': row[5],\n",
    "                'important_considerations': row[6],\n",
    "                'common_joins': row[7],\n",
    "                'example_questions': row[8]\n",
    "            }\n",
    "        \n",
    "        return tables_metadata\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format metadata into text suitable for LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata_for_prompt(metadata):\n",
    "    \"\"\"Format metadata into text suitable for LLM prompt\"\"\"\n",
    "    formatted_text = \"# Database Schema Information\\n\\n\"\n",
    "    \n",
    "    for table_name, table_info in metadata.items():\n",
    "        formatted_text += f\"## Table: {table_name}\\n\"\n",
    "        formatted_text += f\"Description: {table_info['description']}\\n\"\n",
    "        formatted_text += f\"Purpose: {table_info['table_purpose']}\\n\\n\"\n",
    "        \n",
    "        # Add primary key information\n",
    "        if table_info['primary_keys']:\n",
    "            formatted_text += f\"Primary Keys: {', '.join(table_info['primary_keys'])}\\n\\n\"\n",
    "        \n",
    "        # Add foreign key information\n",
    "        if table_info['foreign_keys']:\n",
    "            formatted_text += \"Foreign Keys:\\n\"\n",
    "            for fk_col, fk_info in table_info['foreign_keys'].items():\n",
    "                formatted_text += f\"- {fk_col} references {fk_info['table']}.{fk_info['column']}\\n\"\n",
    "            formatted_text += \"\\n\"\n",
    "        \n",
    "        # Add column information\n",
    "        formatted_text += \"Columns:\\n\"\n",
    "        for col_name, col_info in table_info['columns_info'].items():\n",
    "            formatted_text += f\"- {col_name} ({col_info['data_type']}): {col_info['description']}\\n\"\n",
    "            \n",
    "            # Add categorical value distribution if available and not too long\n",
    "            if 'categorical_values' in col_info and len(col_info['categorical_values']) < 15:\n",
    "                formatted_text += f\"  Possible values: {', '.join(col_info['categorical_values'])}\\n\"\n",
    "            \n",
    "            # Add value range if available\n",
    "            if 'value_range' in col_info:\n",
    "                formatted_text += f\"  Range: {col_info['value_range']['min']} to {col_info['value_range']['max']}\\n\"\n",
    "        \n",
    "        formatted_text += \"\\n\"\n",
    "        \n",
    "        # Add important considerations\n",
    "        if table_info['important_considerations']:\n",
    "            formatted_text += f\"Important Considerations: {table_info['important_considerations']}\\n\\n\"\n",
    "        \n",
    "        # Add common joins\n",
    "        if table_info['common_joins']:\n",
    "            formatted_text += \"Common Joins:\\n\"\n",
    "            for join in table_info['common_joins']:\n",
    "                formatted_text += f\"- {join}\\n\"\n",
    "            formatted_text += \"\\n\"\n",
    "        \n",
    "        formatted_text += \"---\\n\\n\"\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch metadata for tables most relevant to the query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_relevant_metadata(query_embedding, top_k):\n",
    "    \"\"\"\n",
    "    Fetch metadata for tables most relevant to the query embedding\n",
    "    \n",
    "    Parameters:\n",
    "        query_embedding (list): Vector representation of the user query\n",
    "        top_k (int): Number of most relevant tables to return\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping table names to their metadata with similarity scores\n",
    "    \"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Computing similarity to find top {top_k} relevant tables\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                table_name, description, table_purpose, columns_info, \n",
    "                primary_keys, foreign_keys, important_considerations,\n",
    "                common_joins, example_questions, embedding\n",
    "            FROM mimic_table_metadata\n",
    "            WHERE embedding IS NOT NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        rows = cur.fetchall()\n",
    "        \n",
    "        # Calculate similarity for each table\n",
    "        table_similarities = []\n",
    "        for row in rows:\n",
    "            table_name = row[0]\n",
    "            table_embedding = row[9]\n",
    "            \n",
    "            # Skip if embedding is NULL\n",
    "            if table_embedding is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert string embedding to list of floats if needed\n",
    "            if isinstance(table_embedding, str):\n",
    "                import json\n",
    "                table_embedding = json.loads(table_embedding.replace(\"'\", '\"'))\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(query_embedding, table_embedding)\n",
    "            table_similarities.append((row, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending) and take top_k\n",
    "        table_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_tables = table_similarities[:top_k]\n",
    "        \n",
    "        # Format as dictionary\n",
    "        tables_metadata = {}\n",
    "        for row, similarity in top_tables:\n",
    "            table_name = row[0]\n",
    "            tables_metadata[table_name] = {\n",
    "                'description': row[1],\n",
    "                'table_purpose': row[2],\n",
    "                'columns_info': row[3],\n",
    "                'primary_keys': row[4],\n",
    "                'foreign_keys': row[5],\n",
    "                'important_considerations': row[6],\n",
    "                'common_joins': row[7],\n",
    "                'example_questions': row[8],\n",
    "                'similarity_score': similarity  # Add similarity score to metadata\n",
    "            }\n",
    "            \n",
    "            # Print similarity for debugging\n",
    "            print(f\"Table: {table_name}, Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        return tables_metadata\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute cosine similarity between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors\n",
    "    \n",
    "    Parameters:\n",
    "        vec1 (list): First vector\n",
    "        vec2 (list): Second vector\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity (between -1 and 1)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the prompt to send to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompt(user_question, metadata_text):\n",
    "    \"\"\"Create the complete prompt to send to the LLM with improved instructions\"\"\"\n",
    "    # Add explicit column information to the prompt\n",
    "    prompt = f\"\"\"You are a professional medical database expert specializing in SQL and the MIMIC-IV database. Based on the user's question and the provided database metadata, generate a PostgreSQL query.\n",
    "\n",
    "## User Question\n",
    "{user_question}\n",
    "\n",
    "## Database Metadata\n",
    "{metadata_text}\n",
    "\n",
    "## Task\n",
    "1. Analyze the user question to determine which tables and columns need to be queried\n",
    "2. Design an effective SQL query based on the provided metadata\n",
    "3. Ensure the generated SQL is syntactically correct and considers table relationships\n",
    "4. Use ONLY columns that are explicitly mentioned in the metadata for each table\n",
    "5. If multiple table joins are needed, use the correct join conditions\n",
    "6. Handle any potential edge cases\n",
    "7. When dealing with medical codes (ICD diagnosis/procedure codes, medication codes), always join with their respective descriptor tables (d_icd_diagnoses, d_icd_procedures) to include both codes AND their human-readable descriptions\n",
    "8. For medications, include the actual drug names from prescriptions.drug or emar.medication rather than just codes\n",
    "\n",
    "## Response Format\n",
    "Please return ONLY the SQL query without any explanation or comments. Start your answer with \"SELECT\" or \"WITH\" and end with a semicolon. Do not include anything else.\n",
    "\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SQL query using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_with_openai(prompt):\n",
    "    \"\"\"Generate SQL query using OpenAI API with improved cleaning and validation\"\"\"\n",
    "    try:\n",
    "        # Using the new client format\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # or another suitable model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical database expert who converts natural language questions into PostgreSQL queries. Return ONLY the SQL query with no explanations or comments.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,  # Low temperature for more deterministic output\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Get raw content\n",
    "        raw_content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # More comprehensive cleaning of markdown and prefixes\n",
    "        # Remove common prefixes\n",
    "        prefixes = [\"SQL Query:\", \"Query:\", \"PostgreSQL Query:\"]\n",
    "        for prefix in prefixes:\n",
    "            if raw_content.startswith(prefix):\n",
    "                raw_content = raw_content[len(prefix):].strip()\n",
    "        \n",
    "        # Remove markdown code blocks (handling various formats)\n",
    "        import re\n",
    "        sql_query = re.sub(r'```(?:sql|postgresql)?|```', '', raw_content)\n",
    "        sql_query = sql_query.strip()\n",
    "        \n",
    "        # Find the termination point of the SQL part - look for typical SQL statement ending (semicolon) followed by a newline\n",
    "        # This will remove explanatory text after the query\n",
    "        match = re.search(r';[\\s\\n]*(\\n|$)', sql_query)\n",
    "        if match:\n",
    "            # Only keep the part up to the semicolon\n",
    "            sql_query = sql_query[:match.end()].strip()\n",
    "        \n",
    "        # Basic SQL syntax validation\n",
    "        if not sql_query.lower().startswith(('select', 'with')):\n",
    "            print(\"Warning: Generated SQL may not be valid. It doesn't start with SELECT or WITH.\")\n",
    "            \n",
    "        # Log the cleaned query for debugging\n",
    "        print(f\"Cleaned SQL query: {sql_query[:100]}...\")\n",
    "        \n",
    "        return sql_query\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_table_structure(table_name):\n",
    "    \"\"\"Get the actual column structure of a table, returns a list of column names\"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        # Get column names for the table\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = '{table_name}'\n",
    "        \"\"\")\n",
    "        \n",
    "        columns = [row[0] for row in cur.fetchall()]\n",
    "        print(f\"Columns in table {table_name}: {', '.join(columns)}\")\n",
    "        return columns\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting structure for table {table_name}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def check_query_columns(sql_query):\n",
    "    \"\"\"Analyze SQL query, validate that all referenced tables and columns exist\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Extract tables used in the query\n",
    "    from_pattern = re.compile(r'\\bFROM\\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "    join_pattern = re.compile(r'\\bJOIN\\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "    \n",
    "    tables = from_pattern.findall(sql_query) + join_pattern.findall(sql_query)\n",
    "    tables = list(set(tables))  # Remove duplicates\n",
    "    \n",
    "    # Get the actual column structure for each table\n",
    "    table_columns = {}\n",
    "    for table in tables:\n",
    "        table_columns[table] = validate_table_structure(table)\n",
    "    \n",
    "    # A simple method to find column references in the query\n",
    "    # This is a simplified version; a complete SQL parser would be needed for full accuracy\n",
    "    potential_issues = []\n",
    "    \n",
    "    for table in tables:\n",
    "        columns = table_columns[table]\n",
    "        # Look for patterns like \"table.column\"\n",
    "        table_column_pattern = re.compile(rf'\\b{table}\\.([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "        referenced_columns = table_column_pattern.findall(sql_query)\n",
    "        \n",
    "        for col in referenced_columns:\n",
    "            if col not in columns:\n",
    "                potential_issues.append(f\"Warning: Column '{col}' does not exist in table '{table}'\")\n",
    "    \n",
    "    return potential_issues, table_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to automatically fix column reference issues in the SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attempt_fix_sql(sql_query, table_columns):\n",
    "    \"\"\"Attempt to automatically fix column reference issues in the SQL query\"\"\"\n",
    "    prompt = f\"\"\"As a database expert, please fix the following SQL query to make it compatible with the provided table structures.\n",
    "    \n",
    "SQL query:\n",
    "```sql\n",
    "{sql_query}\n",
    "```\n",
    "\n",
    "Table structure information:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add table structure information\n",
    "    for table, columns in table_columns.items():\n",
    "        prompt += f\"\\nTable '{table}' columns: {', '.join(columns)}\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "\n",
    "Please provide the fixed SQL query, ensuring that:\n",
    "1. Only use columns that exist in the tables\n",
    "2. Fix any mismatches in join conditions\n",
    "3. Maintain the basic logic and purpose of the query\n",
    "4. Return only the fixed SQL query, without any explanations or comments\n",
    "\n",
    "Fixed SQL query:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an SQL expert, specializing in fixing errors in SQL queries.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        fixed_sql = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean formatting\n",
    "        import re\n",
    "        fixed_sql = re.sub(r'```(?:sql|postgresql)?|```', '', fixed_sql)\n",
    "        fixed_sql = fixed_sql.strip()\n",
    "        \n",
    "        return fixed_sql\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while attempting to fix SQL: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute SQL query and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_query(sql_query, timeout_seconds=3000):\n",
    "    \"\"\"Execute SQL query with timeout and improved error handling\"\"\"\n",
    "    conn = None\n",
    "    cur = None\n",
    "    \n",
    "    try:\n",
    "        # Get a fresh connection\n",
    "        conn = psycopg2.connect(**DB_PARAMS)\n",
    "        \n",
    "        # Enable autocommit for session parameter changes\n",
    "        conn.autocommit = True\n",
    "        \n",
    "        # Create cursor\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Set statement timeout before starting transaction\n",
    "        cur.execute(f\"SET statement_timeout = {timeout_seconds * 1000};\")  # milliseconds\n",
    "        \n",
    "        # Switch to transaction mode for the actual query\n",
    "        conn.autocommit = False\n",
    "        \n",
    "        # Log the query being executed\n",
    "        print(f\"Executing SQL (with {timeout_seconds}s timeout): {sql_query[:200]}...\")\n",
    "        \n",
    "        # Execute the query\n",
    "        cur.execute(sql_query)\n",
    "        \n",
    "        # Get column names if the query returns results\n",
    "        if cur.description:\n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            \n",
    "            # Fetch results with a row limit to avoid memory issues\n",
    "            results = []\n",
    "            while True:\n",
    "                batch = cur.fetchmany(1000)  # Fetch in batches\n",
    "                if not batch:\n",
    "                    break\n",
    "                results.extend(batch)\n",
    "                \n",
    "                # Check if we've fetched enough rows\n",
    "                if len(results) >= 10000:  # Set a reasonable maximum\n",
    "                    print(\"Warning: Query returned more than 10,000 rows, truncating results\")\n",
    "                    break\n",
    "            \n",
    "            # Commit transaction\n",
    "            conn.commit()\n",
    "            \n",
    "            # Convert results to DataFrame\n",
    "            df = pd.DataFrame(results, columns=column_names)\n",
    "            \n",
    "            print(f\"Query returned {len(df)} rows and {len(df.columns)} columns\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            # For queries that don't return results (e.g., INSERT, UPDATE)\n",
    "            conn.commit()\n",
    "            print(\"Query executed successfully (no results returned)\")\n",
    "            return pd.DataFrame()  # Empty DataFrame\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        if conn:\n",
    "            try:\n",
    "                conn.rollback()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        error_msg = f\"Error executing SQL query: {e}\"\n",
    "        print(error_msg)\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        if cur:\n",
    "            try:\n",
    "                # Reset statement timeout if possible\n",
    "                if conn and conn.status == psycopg2.extensions.STATUS_READY:\n",
    "                    conn.autocommit = True\n",
    "                    cur.execute(\"RESET statement_timeout;\")\n",
    "            except:\n",
    "                pass\n",
    "            cur.close()\n",
    "        \n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a user's natural language query into a vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_user_query(query_text):\n",
    "    \"\"\"\n",
    "    Convert a user's natural language query into a vector representation\n",
    "    \n",
    "    Parameters:\n",
    "        query_text (str): The user's natural language query\n",
    "        \n",
    "    Returns:\n",
    "        list: Vector representation of the query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the query text - more processing steps can be added as needed\n",
    "        processed_query = query_text.strip()\n",
    "        \n",
    "        # Generate embedding vector using OpenAI API\n",
    "        response = client.embeddings.create(\n",
    "            input=processed_query,\n",
    "            model=\"text-embedding-3-small\"  # Use the same model as for table embeddings\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding vector\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        print(f\"✅ Successfully vectorized query: '{query_text[:50]}...' if len(query_text) > 50 else query_text\")\n",
    "        return query_embedding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error vectorizing query: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format a simplified version of metadata for the answer generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata_for_answer(metadata):\n",
    "    \"\"\"Format a simplified version of metadata for the answer generation prompt\"\"\"\n",
    "    formatted_text = \"\"\n",
    "    \n",
    "    for table_name, table_info in metadata.items():\n",
    "        formatted_text += f\"Table: {table_name}\\n\"\n",
    "        formatted_text += f\"Description: {table_info['description']}\\n\\n\"\n",
    "        \n",
    "        # Add key columns (simplified)\n",
    "        formatted_text += \"Key columns:\\n\"\n",
    "        for col_name, col_info in table_info['columns_info'].items():\n",
    "            if col_name in table_info.get('primary_keys', []) or 'key' in col_name.lower():\n",
    "                formatted_text += f\"- {col_name}: {col_info['description']}\\n\"\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a comprehensive natural language answer based on query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_natural_language_answer(user_question, metadata, sql_query, query_results):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive natural language answer based on query results\n",
    "    \n",
    "    Parameters:\n",
    "        user_question (str): Original user question\n",
    "        metadata (dict): Metadata of relevant tables used in the query\n",
    "        sql_query (str): Generated SQL query\n",
    "        query_results: DataFrame or other format containing query results\n",
    "        \n",
    "    Returns:\n",
    "        str: Natural language answer explaining the results\n",
    "    \"\"\"\n",
    "    # Convert query results to a suitable format for LLM\n",
    "    if isinstance(query_results, pd.DataFrame):\n",
    "        # For large DataFrames, include sample and summary statistics\n",
    "        if len(query_results) > 10:\n",
    "            results_text = f\"Results contain {len(query_results)} rows.\\n\\n\"\n",
    "            results_text += \"Sample of first 5 rows:\\n\"\n",
    "            results_text += query_results.head(5).to_string() + \"\\n\\n\"\n",
    "            \n",
    "            # Add summary statistics if numerical columns exist\n",
    "            if any(query_results.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n",
    "                results_text += \"Summary statistics:\\n\"\n",
    "                results_text += query_results.describe().to_string()\n",
    "        else:\n",
    "            results_text = \"Results:\\n\" + query_results.to_string()\n",
    "    else:\n",
    "        results_text = f\"Results: {str(query_results)}\"\n",
    "    \n",
    "    # Create a comprehensive prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    User question: {user_question}\n",
    "    \n",
    "    Database information used:\n",
    "    {format_metadata_for_answer(metadata)}\n",
    "    \n",
    "    SQL query executed:\n",
    "    ```sql\n",
    "    {sql_query}\n",
    "    ```\n",
    "    \n",
    "    {results_text}\n",
    "    \n",
    "    Based on the above information, please provide a comprehensive answer to the user's question.\n",
    "    Explain the results in natural language, highlighting key insights, patterns, or important values.\n",
    "    If appropriate, suggest any follow-up analyses that might be valuable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the LLM to generate the answer\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains database query results clearly.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classify the query to identify if it's answerable with available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(user_question, metadata):\n",
    "    \"\"\"\n",
    "    Classify the query to identify if it's answerable with available data\n",
    "    \n",
    "    Parameters:\n",
    "        user_question (str): The user's question\n",
    "        metadata (dict): Metadata of most relevant tables\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classification results including status and reason\n",
    "    \"\"\"\n",
    "    # Create a prompt for query classification\n",
    "    tables_summary = \"\\n\".join([f\"- {table}: {info['description']}\" \n",
    "                              for table, info in metadata.items()])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on the following database tables from a MEDICAL DATABASE (MIMIC-IV 2.2), \n",
    "    classify if this question can be answered:\n",
    "    \n",
    "    USER QUESTION: {user_question}\n",
    "    \n",
    "    AVAILABLE TABLES:\n",
    "    {tables_summary}\n",
    "    \n",
    "    Please classify this question as one of:\n",
    "    1. \"answerable\": Can be answered with the available tables\n",
    "    2. \"out_of_scope\": Relates to medical data but not available in these tables\n",
    "    3. \"non_medical\": Not related to medical data at all\n",
    "    4. \"future_data\": Requires data from after the database collection period\n",
    "    5. \"private_data\": Asks for personally identifiable information\n",
    "    \n",
    "    You must respond in valid JSON format with exactly these fields:\n",
    "    {{\n",
    "      \"status\": \"one of the options above\",\n",
    "      \"reason\": \"brief explanation why you classified it this way\",\n",
    "      \"message\": \"user-friendly message explaining if/why the question can't be answered\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call LLM to classify, without specifying response_format\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies database queries. Always respond in valid JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    import json\n",
    "    try:\n",
    "        classification = json.loads(response.choices[0].message.content)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback in case the response isn't valid JSON\n",
    "        return {\n",
    "            \"classification\": \"answerable\",  # Default to answerable\n",
    "            \"reason\": \"Failed to parse classification response\",\n",
    "            \"message\": \"I'll try to answer your question with the available data.\",\n",
    "            \"status\": \"supported\"\n",
    "        }\n",
    "    \n",
    "    # Map the classification to pipeline control values\n",
    "    result = {\n",
    "        \"classification\": classification.get(\"status\", \"answerable\"),\n",
    "        \"reason\": classification.get(\"reason\", \"No reason provided\"),\n",
    "        \"message\": classification.get(\"message\", \"No message provided\")\n",
    "    }\n",
    "    \n",
    "    # Set overall status\n",
    "    if result[\"classification\"] == \"answerable\":\n",
    "        result[\"status\"] = \"supported\"\n",
    "    else:\n",
    "        result[\"status\"] = \"not_supported\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the highest similarity score and corresponding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_similarity(metadata):\n",
    "    \"\"\"Extract the highest similarity score and corresponding table\"\"\"\n",
    "    best_table = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for table_name, info in metadata.items():\n",
    "        if \"similarity_score\" in info and info[\"similarity_score\"] > best_score:\n",
    "            best_score = info[\"similarity_score\"]\n",
    "            best_table = table_name\n",
    "    \n",
    "    return best_table, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlrag_pipeline(user_question):\n",
    "    \"\"\"Execute the complete SQLRAG pipeline with enhanced safety checks\"\"\"\n",
    "    print(f\"User Question: {user_question}\")\n",
    "    \n",
    "    # 1. Vectorize user query\n",
    "    print(\"Vectorizing user query...\")\n",
    "    query_embedding = vectorize_user_query(user_question)\n",
    "    \n",
    "    if not query_embedding:\n",
    "        return {\"error\": \"Failed to vectorize user query\"}\n",
    "    \n",
    "    # 2. Fetch relevant metadata using vector similarity\n",
    "    print(\"Finding relevant tables...\")\n",
    "    metadata = fetch_relevant_metadata(query_embedding, top_k=10)\n",
    "    \n",
    "    # 3. Check if any relevant tables were found with good similarity\n",
    "    if not metadata:\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": \"No relevant tables found in the database for this query.\",\n",
    "            \"answer\": \"I don't have the necessary data to answer this question. The database doesn't contain information related to your query.\"\n",
    "        }\n",
    "    \n",
    "    # 4. Check similarity scores to ensure they're above threshold\n",
    "    # Get the highest similarity score\n",
    "    best_table, best_similarity = get_highest_similarity(metadata)\n",
    "    if best_similarity < 0.2:  # Adjust threshold as needed\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"best_match\": best_table,\n",
    "            \"similarity\": best_similarity,\n",
    "            \"error\": \"The query doesn't seem to match well with available data.\",\n",
    "            \"answer\": f\"Your question might not be answerable with the available medical data. The closest match I found was related to '{best_table}' but the relevance is low.\"\n",
    "        }\n",
    "    \n",
    "    # 5. Use query classifier to identify query intent and feasibility\n",
    "    query_classification = classify_query(user_question, metadata)\n",
    "    if query_classification[\"status\"] == \"not_supported\":\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": query_classification[\"reason\"],\n",
    "            \"answer\": query_classification[\"message\"]\n",
    "        }\n",
    "    \n",
    "    # 6. Format metadata for prompt - ensure metadata includes complete column information\n",
    "    metadata_text = format_metadata_for_prompt(metadata)\n",
    "    \n",
    "    # 7. Create LLM prompt with improved instructions\n",
    "    prompt = create_llm_prompt(user_question, metadata_text)\n",
    "    \n",
    "    print(\"Generating SQL with LLM...\")\n",
    "    # 8. Generate SQL query\n",
    "    sql_query = generate_sql_with_openai(prompt)\n",
    "    \n",
    "    if not sql_query:\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": \"Failed to generate SQL query\",\n",
    "            \"answer\": \"I couldn't generate a SQL query to answer your question. Please try rephrasing it.\"\n",
    "        }\n",
    "    \n",
    "    print(f\"Generated SQL: \\n{sql_query}\\n\")\n",
    "    \n",
    "    # 9. New step: Validate the generated SQL against table structure\n",
    "    print(\"Validating SQL against database structure...\")\n",
    "    issues, table_columns = check_query_columns(sql_query)\n",
    "    \n",
    "    if issues:\n",
    "        print(\"Potential issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        \n",
    "        # Try to automatically fix the SQL\n",
    "        print(\"Attempting to fix SQL...\")\n",
    "        fixed_sql = attempt_fix_sql(sql_query, table_columns)\n",
    "        \n",
    "        if fixed_sql:\n",
    "            print(f\"Fixed SQL: \\n{fixed_sql}\\n\")\n",
    "            sql_query = fixed_sql\n",
    "        else:\n",
    "            return {\n",
    "                \"user_question\": user_question, \n",
    "                \"error\": f\"Generated SQL query is incompatible with database structure: {'; '.join(issues)}\",\n",
    "                \"generated_sql\": sql_query,\n",
    "                \"answer\": \"I couldn't generate a valid query compatible with the database structure. There might be a mismatch in my understanding of the database schema.\"\n",
    "            }\n",
    "    \n",
    "    print(\"Executing SQL query...\")\n",
    "    # 10. Execute SQL query\n",
    "    results = execute_sql_query(sql_query)\n",
    "    \n",
    "    if results is None:\n",
    "        return {\n",
    "            \"user_question\": user_question,\n",
    "            \"error\": \"Error executing SQL query\",\n",
    "            \"generated_sql\": sql_query,\n",
    "            \"answer\": \"I apologize, but I couldn't execute the generated SQL query. There might be an issue with the database or the query structure.\"\n",
    "        }\n",
    "    \n",
    "    print(\"Generating natural language answer...\")\n",
    "    # 11. Generate natural language answer\n",
    "    answer = generate_natural_language_answer(user_question, metadata, sql_query, results)\n",
    "    \n",
    "    return {\n",
    "        \"user_question\": user_question,\n",
    "        \"generated_sql\": sql_query,\n",
    "        \"results\": results,\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_from_sql(sql_query):\n",
    "    \"\"\"Extract table names used in a SQL query\"\"\"\n",
    "    # Simple regex-based extraction - could be enhanced\n",
    "    import re\n",
    "    from_pattern = re.compile(r'\\bFROM\\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "    join_pattern = re.compile(r'\\bJOIN\\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)\n",
    "    \n",
    "    tables = from_pattern.findall(sql_query) + join_pattern.findall(sql_query)\n",
    "    return list(set(tables))  # Remove duplicates\n",
    "\n",
    "def measure_execution_time(sql_query):\n",
    "    \"\"\"Measure execution time of a SQL query\"\"\"\n",
    "    import time\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        cur.execute(sql_query)\n",
    "        results = cur.fetchall()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return end_time - start_time\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comprehensive_sqlrag():\n",
    "    \"\"\"Test the enhanced SQLRAG pipeline with a complex medical query\"\"\"\n",
    "    \n",
    "    # Complex query that requires multiple table joining and medical domain knowledge\n",
    "    complex_query = \"\"\"\n",
    "    Which patients had the highest number of hospital readmissions within 30 days of discharge, \n",
    "    and what were their most common diagnoses and prescribed medications? \n",
    "    Also analyze if there's any correlation between length of stay and readmission rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE TEST CASE FOR MIMIC-IV 2.2\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(f\"QUERY: {complex_query}\\n\")\n",
    "    \n",
    "    # Run enhanced SQLRAG pipeline\n",
    "    result = sqlrag_pipeline(complex_query)\n",
    "    \n",
    "    # Check for errors in the result\n",
    "    if \"error\" in result:\n",
    "        print(\"\\nERROR DETECTED:\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        print(f\"Answer: {result.get('answer', 'No answer provided')}\")\n",
    "        return result\n",
    "    \n",
    "    if isinstance(result, str):\n",
    "        print(\"\\nERROR AS STRING:\")\n",
    "        print(\"-\"*50)\n",
    "        print(result)\n",
    "        \n",
    "    result = {\n",
    "        \"error\": result,\n",
    "        \"answer\": \"An error occurred during query processing.\"\n",
    "    }\n",
    "\n",
    "    # Print the answer\n",
    "    print(\"\\nNATURAL LANGUAGE ANSWER:\")\n",
    "    print(\"-\"*50)\n",
    "    print(result.get(\"answer\", \"No answer generated\"))\n",
    "    \n",
    "    # Only continue with analysis if we have SQL and results\n",
    "    if \"generated_sql\" in result and \"results\" in result:\n",
    "        print(\"\\nQUERY QUALITY ANALYSIS:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # 1. Check tables used in SQL\n",
    "        tables_used = extract_tables_from_sql(result[\"generated_sql\"])\n",
    "        print(f\"Tables used in query: {', '.join(tables_used)}\")\n",
    "        \n",
    "        # 2. Execution metrics\n",
    "        execution_time = measure_execution_time(result[\"generated_sql\"])\n",
    "        print(f\"Query execution time: {execution_time:.2f} seconds\")\n",
    "        \n",
    "        # 3. Result size check\n",
    "        if isinstance(result[\"results\"], pd.DataFrame):\n",
    "            print(f\"Result size: {len(result['results'])} rows, {len(result['results'].columns)} columns\")\n",
    "    else:\n",
    "        print(\"\\nSKIPPING QUERY ANALYSIS: No SQL or results available\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE TEST CASE FOR MIMIC-IV 2.2\n",
      "================================================================================\n",
      "\n",
      "QUERY: \n",
      "    Which patients had the highest number of hospital readmissions within 30 days of discharge, \n",
      "    and what were their most common diagnoses and prescribed medications? \n",
      "    Also analyze if there's any correlation between length of stay and readmission rates.\n",
      "    \n",
      "\n",
      "User Question: \n",
      "    Which patients had the highest number of hospital readmissions within 30 days of discharge, \n",
      "    and what were their most common diagnoses and prescribed medications? \n",
      "    Also analyze if there's any correlation between length of stay and readmission rates.\n",
      "    \n",
      "Vectorizing user query...\n",
      "✅ Successfully vectorized query: '\n",
      "    Which patients had the highest number of hosp...' if len(query_text) > 50 else query_text\n",
      "Finding relevant tables...\n",
      "Computing similarity to find top 10 relevant tables\n",
      "Table: admissions, Similarity: 0.4974\n",
      "Table: diagnoses_icd, Similarity: 0.4940\n",
      "Table: transfers, Similarity: 0.4487\n",
      "Table: drgcodes, Similarity: 0.4329\n",
      "Table: procedures_icd, Similarity: 0.4304\n",
      "Table: prescriptions, Similarity: 0.4163\n",
      "Table: services, Similarity: 0.4161\n",
      "Table: hcpcsevents, Similarity: 0.4130\n",
      "Table: pharmacy, Similarity: 0.4042\n",
      "Table: emar, Similarity: 0.4012\n",
      "Generating SQL with LLM...\n",
      "Cleaned SQL query: WITH readmissions AS (\n",
      "    SELECT a1.subject_id, COUNT(*) AS readmission_count\n",
      "    FROM admissions a...\n",
      "Generated SQL: \n",
      "WITH readmissions AS (\n",
      "    SELECT a1.subject_id, COUNT(*) AS readmission_count\n",
      "    FROM admissions a1\n",
      "    JOIN admissions a2 ON a1.subject_id = a2.subject_id\n",
      "    WHERE a2.admittime BETWEEN a1.dischtime AND a1.dischtime + INTERVAL '30 days'\n",
      "    GROUP BY a1.subject_id\n",
      "),\n",
      "diagnoses AS (\n",
      "    SELECT d.subject_id, d.icd_code, COUNT(*) AS diagnosis_count\n",
      "    FROM diagnoses_icd d\n",
      "    JOIN readmissions r ON d.subject_id = r.subject_id\n",
      "    GROUP BY d.subject_id, d.icd_code\n",
      "),\n",
      "medications AS (\n",
      "    SELECT p.subject_id, p.drug, COUNT(*) AS medication_count\n",
      "    FROM prescriptions p\n",
      "    JOIN readmissions r ON p.subject_id = r.subject_id\n",
      "    GROUP BY p.subject_id, p.drug\n",
      "),\n",
      "stay_length AS (\n",
      "    SELECT a.subject_id, AVG(EXTRACT(EPOCH FROM a.dischtime - a.admittime)/3600) AS avg_stay_length\n",
      "    FROM admissions a\n",
      "    JOIN readmissions r ON a.subject_id = r.subject_id\n",
      "    GROUP BY a.subject_id\n",
      ")\n",
      "SELECT r.subject_id, r.readmission_count, d.icd_code, d.diagnosis_count, m.drug, m.medication_count, s.avg_stay_length\n",
      "FROM readmissions r\n",
      "JOIN diagnoses d ON r.subject_id = d.subject_id\n",
      "JOIN medications m ON r.subject_id = m.subject_id\n",
      "JOIN stay_length s ON r.subject_id = s.subject_id\n",
      "ORDER BY r.readmission_count DESC, d.diagnosis_count DESC, m.medication_count DESC;\n",
      "\n",
      "Validating SQL against database structure...\n",
      "Columns in table admissions: subject_id, hadm_id, admittime, dischtime, deathtime, edregtime, edouttime, hospital_expire_flag, discharge_location, insurance, language, marital_status, race, admission_type, admit_provider_id, admission_location\n",
      "Columns in table diagnoses: \n",
      "Columns in table diagnoses_icd: subject_id, hadm_id, seq_num, icd_version, icd_code\n",
      "Columns in table prescriptions: poe_seq, pharmacy_id, starttime, stoptime, hadm_id, doses_per_24_hrs, subject_id, ndc, prod_strength, form_rx, dose_val_rx, dose_unit_rx, form_val_disp, form_unit_disp, route, poe_id, order_provider_id, drug_type, drug, formulary_drug_cd, gsn\n",
      "Columns in table readmissions: \n",
      "Columns in table stay_length: \n",
      "Columns in table a: \n",
      "Columns in table medications: \n",
      "Potential issues detected:\n",
      "  - Warning: Column 'subject_id' does not exist in table 'a'\n",
      "  - Warning: Column 'dischtime' does not exist in table 'a'\n",
      "  - Warning: Column 'admittime' does not exist in table 'a'\n",
      "  - Warning: Column 'subject_id' does not exist in table 'a'\n",
      "  - Warning: Column 'subject_id' does not exist in table 'a'\n",
      "Attempting to fix SQL...\n",
      "Fixed SQL: \n",
      "WITH readmissions AS (\n",
      "    SELECT a1.subject_id, COUNT(*) AS readmission_count\n",
      "    FROM admissions a1\n",
      "    JOIN admissions a2 ON a1.subject_id = a2.subject_id AND a1.hadm_id <> a2.hadm_id\n",
      "    WHERE a2.admittime BETWEEN a1.dischtime AND a1.dischtime + INTERVAL '30 days'\n",
      "    GROUP BY a1.subject_id\n",
      "),\n",
      "diagnoses AS (\n",
      "    SELECT d.subject_id, d.icd_code, COUNT(*) AS diagnosis_count\n",
      "    FROM diagnoses_icd d\n",
      "    JOIN readmissions r ON d.subject_id = r.subject_id\n",
      "    GROUP BY d.subject_id, d.icd_code\n",
      "),\n",
      "medications AS (\n",
      "    SELECT p.subject_id, p.drug, COUNT(*) AS medication_count\n",
      "    FROM prescriptions p\n",
      "    JOIN readmissions r ON p.subject_id = r.subject_id\n",
      "    GROUP BY p.subject_id, p.drug\n",
      "),\n",
      "stay_length AS (\n",
      "    SELECT a.subject_id, AVG(EXTRACT(EPOCH FROM a.dischtime - a.admittime)/3600) AS avg_stay_length\n",
      "    FROM admissions a\n",
      "    JOIN readmissions r ON a.subject_id = r.subject_id\n",
      "    GROUP BY a.subject_id\n",
      ")\n",
      "SELECT r.subject_id, r.readmission_count, d.icd_code, d.diagnosis_count, m.drug, m.medication_count, s.avg_stay_length\n",
      "FROM readmissions r\n",
      "JOIN diagnoses d ON r.subject_id = d.subject_id\n",
      "JOIN medications m ON r.subject_id = m.subject_id\n",
      "JOIN stay_length s ON r.subject_id = s.subject_id\n",
      "ORDER BY r.readmission_count DESC, d.diagnosis_count DESC, m.medication_count DESC;\n",
      "\n",
      "Executing SQL query...\n",
      "Executing SQL (with 3000s timeout): WITH readmissions AS (\n",
      "    SELECT a1.subject_id, COUNT(*) AS readmission_count\n",
      "    FROM admissions a1\n",
      "    JOIN admissions a2 ON a1.subject_id = a2.subject_id AND a1.hadm_id <> a2.hadm_id\n",
      "    WHERE a2....\n",
      "Error executing SQL query: could not write to file \"base/pgsql_tmp/pgsql_tmp2153.6\": No space left on device\n",
      "\n",
      "\n",
      "ERROR DETECTED:\n",
      "--------------------------------------------------\n",
      "Error: Error executing SQL query\n",
      "Answer: I apologize, but I couldn't execute the generated SQL query. There might be an issue with the database or the query structure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_question': \"\\n    Which patients had the highest number of hospital readmissions within 30 days of discharge, \\n    and what were their most common diagnoses and prescribed medications? \\n    Also analyze if there's any correlation between length of stay and readmission rates.\\n    \",\n",
       " 'error': 'Error executing SQL query',\n",
       " 'generated_sql': \"WITH readmissions AS (\\n    SELECT a1.subject_id, COUNT(*) AS readmission_count\\n    FROM admissions a1\\n    JOIN admissions a2 ON a1.subject_id = a2.subject_id AND a1.hadm_id <> a2.hadm_id\\n    WHERE a2.admittime BETWEEN a1.dischtime AND a1.dischtime + INTERVAL '30 days'\\n    GROUP BY a1.subject_id\\n),\\ndiagnoses AS (\\n    SELECT d.subject_id, d.icd_code, COUNT(*) AS diagnosis_count\\n    FROM diagnoses_icd d\\n    JOIN readmissions r ON d.subject_id = r.subject_id\\n    GROUP BY d.subject_id, d.icd_code\\n),\\nmedications AS (\\n    SELECT p.subject_id, p.drug, COUNT(*) AS medication_count\\n    FROM prescriptions p\\n    JOIN readmissions r ON p.subject_id = r.subject_id\\n    GROUP BY p.subject_id, p.drug\\n),\\nstay_length AS (\\n    SELECT a.subject_id, AVG(EXTRACT(EPOCH FROM a.dischtime - a.admittime)/3600) AS avg_stay_length\\n    FROM admissions a\\n    JOIN readmissions r ON a.subject_id = r.subject_id\\n    GROUP BY a.subject_id\\n)\\nSELECT r.subject_id, r.readmission_count, d.icd_code, d.diagnosis_count, m.drug, m.medication_count, s.avg_stay_length\\nFROM readmissions r\\nJOIN diagnoses d ON r.subject_id = d.subject_id\\nJOIN medications m ON r.subject_id = m.subject_id\\nJOIN stay_length s ON r.subject_id = s.subject_id\\nORDER BY r.readmission_count DESC, d.diagnosis_count DESC, m.medication_count DESC;\",\n",
       " 'answer': \"I apologize, but I couldn't execute the generated SQL query. There might be an issue with the database or the query structure.\"}"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comprehensive_sqlrag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_complex_medical_analysis():\n",
    "    \"\"\"Test SQLRAG with a complex medical analysis query\"\"\"\n",
    "    \n",
    "    complex_query = \"\"\"\n",
    "    For patients diagnosed with sepsis (ICD code 99591 or A41.9), \n",
    "    what is the average length of ICU stay, mortality rate, \n",
    "    and what are the top 5 most commonly administered medications?\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLEX MEDICAL ANALYSIS TEST CASE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(f\"QUERY: {complex_query}\\n\")\n",
    "    \n",
    "    # Run the SQLRAG pipeline\n",
    "    result = sqlrag_pipeline(complex_query)\n",
    "    \n",
    "    # Handle different return types\n",
    "    if isinstance(result, str):\n",
    "        print(\"\\nERROR OR STRING RESULT:\")\n",
    "        print(\"-\"*50)\n",
    "        print(result)\n",
    "        return result\n",
    "    \n",
    "    # Check for errors in dictionary result\n",
    "    if isinstance(result, dict) and \"error\" in result:\n",
    "        print(\"\\nERROR DETECTED:\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        print(f\"Answer: {result.get('answer', 'No answer provided')}\")\n",
    "        return result\n",
    "    \n",
    "    # Print the results (if dictionary)\n",
    "    if isinstance(result, dict):\n",
    "        print(\"\\nNATURAL LANGUAGE ANSWER:\")\n",
    "        print(\"-\"*50)\n",
    "        print(result.get(\"answer\", \"No answer generated\"))\n",
    "        \n",
    "        # Query analysis\n",
    "        if \"generated_sql\" in result and \"results\" in result:\n",
    "            print(\"\\nQUERY QUALITY ANALYSIS:\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            # Tables used\n",
    "            tables_used = extract_tables_from_sql(result[\"generated_sql\"])\n",
    "            print(f\"Tables used in query: {', '.join(tables_used)}\")\n",
    "            \n",
    "            # Execution metrics\n",
    "            execution_time = measure_execution_time(result[\"generated_sql\"])\n",
    "            print(f\"Query execution time: {execution_time:.2f} seconds\")\n",
    "            \n",
    "            # Results size\n",
    "            if isinstance(result[\"results\"], pd.DataFrame):\n",
    "                print(f\"Result size: {len(result['results'])} rows, {len(result['results'].columns)} columns\")\n",
    "        else:\n",
    "            print(\"\\nSKIPPING QUERY ANALYSIS: No SQL or results available\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLEX MEDICAL ANALYSIS TEST CASE\n",
      "================================================================================\n",
      "\n",
      "QUERY: \n",
      "    For patients diagnosed with sepsis (ICD code 99591 or A41.9), \n",
      "    what is the average length of ICU stay, mortality rate, \n",
      "    and what are the top 5 most commonly administered medications?\n",
      "    \n",
      "\n",
      "User Question: \n",
      "    For patients diagnosed with sepsis (ICD code 99591 or A41.9), \n",
      "    what is the average length of ICU stay, mortality rate, \n",
      "    and what are the top 5 most commonly administered medications?\n",
      "    \n",
      "Vectorizing user query...\n",
      "✅ Successfully vectorized query: '\n",
      "    For patients diagnosed with sepsis (ICD code ...' if len(query_text) > 50 else query_text\n",
      "Finding relevant tables...\n",
      "Computing similarity to find top 10 relevant tables\n",
      "Table: diagnoses_icd, Similarity: 0.4668\n",
      "Table: procedures_icd, Similarity: 0.4274\n",
      "Table: transfers, Similarity: 0.4018\n",
      "Table: d_icd_diagnoses, Similarity: 0.3855\n",
      "Table: hcpcsevents, Similarity: 0.3839\n",
      "Table: services, Similarity: 0.3797\n",
      "Table: drgcodes, Similarity: 0.3590\n",
      "Table: microbiologyevents, Similarity: 0.3550\n",
      "Table: patients, Similarity: 0.3542\n",
      "Table: d_icd_procedures, Similarity: 0.3482\n",
      "Generating SQL with LLM...\n",
      "Cleaned SQL query: WITH sepsis_patients AS (\n",
      "    SELECT subject_id, hadm_id\n",
      "    FROM diagnoses_icd\n",
      "    WHERE icd_code I...\n",
      "Generated SQL: \n",
      "WITH sepsis_patients AS (\n",
      "    SELECT subject_id, hadm_id\n",
      "    FROM diagnoses_icd\n",
      "    WHERE icd_code IN ('99591', 'A419')\n",
      "),\n",
      "icu_stays AS (\n",
      "    SELECT subject_id, hadm_id, AVG(EXTRACT(EPOCH FROM (outtime - intime))/3600) AS avg_icu_length\n",
      "    FROM transfers\n",
      "    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\n",
      "    GROUP BY subject_id, hadm_id\n",
      "),\n",
      "mortality_rate AS (\n",
      "    SELECT COUNT(*) FILTER (WHERE dod IS NOT NULL) * 1.0 / COUNT(*) AS mortality_rate\n",
      "    FROM patients\n",
      "    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\n",
      "),\n",
      "medications AS (\n",
      "    SELECT hcpcs_cd, COUNT(*) AS count\n",
      "    FROM hcpcsevents\n",
      "    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\n",
      "    GROUP BY hcpcs_cd\n",
      "    ORDER BY count DESC\n",
      "    LIMIT 5\n",
      ")\n",
      "SELECT icu_stays.avg_icu_length, mortality_rate.mortality_rate, medications.hcpcs_cd\n",
      "FROM icu_stays, mortality_rate, medications;\n",
      "\n",
      "Validating SQL against database structure...\n",
      "Columns in table transfers: intime, outtime, transfer_id, subject_id, hadm_id, eventtype, careunit\n",
      "Columns in table icu_stays: \n",
      "Columns in table diagnoses_icd: subject_id, hadm_id, seq_num, icd_version, icd_code\n",
      "Columns in table sepsis_patients: \n",
      "Columns in table patients: subject_id, anchor_age, anchor_year, dod, gender, anchor_year_group\n",
      "Columns in table hcpcsevents: subject_id, hadm_id, chartdate, seq_num, hcpcs_cd, short_description\n",
      "Potential issues detected:\n",
      "  - Warning: Column 'avg_icu_length' does not exist in table 'icu_stays'\n",
      "Attempting to fix SQL...\n",
      "Fixed SQL: \n",
      "WITH sepsis_patients AS (\n",
      "    SELECT subject_id, hadm_id\n",
      "    FROM diagnoses_icd\n",
      "    WHERE icd_code IN ('99591', 'A419')\n",
      "),\n",
      "icu_stays AS (\n",
      "    SELECT subject_id, hadm_id, AVG(EXTRACT(EPOCH FROM (outtime - intime))/3600) AS avg_icu_length\n",
      "    FROM transfers\n",
      "    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\n",
      "    GROUP BY subject_id, hadm_id\n",
      "),\n",
      "mortality_rate AS (\n",
      "    SELECT COUNT(*) FILTER (WHERE dod IS NOT NULL) * 1.0 / COUNT(*) AS mortality_rate\n",
      "    FROM patients\n",
      "    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\n",
      "),\n",
      "medications AS (\n",
      "    SELECT hcpcs_cd, COUNT(*) AS count\n",
      "    FROM hcpcsevents\n",
      "    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\n",
      "    GROUP BY hcpcs_cd\n",
      "    ORDER BY count DESC\n",
      "    LIMIT 5\n",
      ")\n",
      "SELECT icu.avg_icu_length, mr.mortality_rate, meds.hcpcs_cd\n",
      "FROM icu_stays icu\n",
      "CROSS JOIN mortality_rate mr\n",
      "CROSS JOIN medications meds;\n",
      "\n",
      "Executing SQL query...\n",
      "Executing SQL (with 3000s timeout): WITH sepsis_patients AS (\n",
      "    SELECT subject_id, hadm_id\n",
      "    FROM diagnoses_icd\n",
      "    WHERE icd_code IN ('99591', 'A419')\n",
      "),\n",
      "icu_stays AS (\n",
      "    SELECT subject_id, hadm_id, AVG(EXTRACT(EPOCH FROM (outtim...\n",
      "Warning: Query returned more than 10,000 rows, truncating results\n",
      "Query returned 10000 rows and 3 columns\n",
      "Generating natural language answer...\n",
      "\n",
      "NATURAL LANGUAGE ANSWER:\n",
      "--------------------------------------------------\n",
      "Based on the query performed, for patients diagnosed with sepsis (ICD code 99591 or A41.9), the average length of stay in the Intensive Care Unit (ICU) is roughly 341 hours, which is approximately 14 days. \n",
      "\n",
      "The mortality rate for these patients is approximately 47.58%. This implies that nearly half of the patients diagnosed with sepsis do not survive, emphasizing the seriousness of this condition.\n",
      "\n",
      "The five most commonly administered medications, based on their HCPCS codes, are G0378, 99219, 99218, 99220, and 43264.\n",
      "\n",
      "The HCPCS codes represent specific services or procedures. However, the results do not provide the actual names or descriptions of these medications. To get a clearer picture, you might consider querying the 'd_hcpcs' table with these codes to fetch the names and descriptions of these medications. \n",
      "\n",
      "Finally, to deepen the analysis, it could be interesting to study how the average length of ICU stay or the mortality rate changes based on different factors like age, gender, or comorbidities. Additionally, studying patterns or changes over time in medication administration could be insightful, possibly revealing changes in treatment strategy or the introduction and effectiveness of new therapies.\n",
      "\n",
      "QUERY QUALITY ANALYSIS:\n",
      "--------------------------------------------------\n",
      "Tables used in query: transfers, icu_stays, diagnoses_icd, sepsis_patients, patients, mortality_rate, hcpcsevents, medications\n",
      "Query execution time: 0.62 seconds\n",
      "Result size: 10000 rows, 3 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_question': '\\n    For patients diagnosed with sepsis (ICD code 99591 or A41.9), \\n    what is the average length of ICU stay, mortality rate, \\n    and what are the top 5 most commonly administered medications?\\n    ',\n",
       " 'generated_sql': \"WITH sepsis_patients AS (\\n    SELECT subject_id, hadm_id\\n    FROM diagnoses_icd\\n    WHERE icd_code IN ('99591', 'A419')\\n),\\nicu_stays AS (\\n    SELECT subject_id, hadm_id, AVG(EXTRACT(EPOCH FROM (outtime - intime))/3600) AS avg_icu_length\\n    FROM transfers\\n    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\\n    GROUP BY subject_id, hadm_id\\n),\\nmortality_rate AS (\\n    SELECT COUNT(*) FILTER (WHERE dod IS NOT NULL) * 1.0 / COUNT(*) AS mortality_rate\\n    FROM patients\\n    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\\n),\\nmedications AS (\\n    SELECT hcpcs_cd, COUNT(*) AS count\\n    FROM hcpcsevents\\n    WHERE subject_id IN (SELECT subject_id FROM sepsis_patients)\\n    GROUP BY hcpcs_cd\\n    ORDER BY count DESC\\n    LIMIT 5\\n)\\nSELECT icu.avg_icu_length, mr.mortality_rate, meds.hcpcs_cd\\nFROM icu_stays icu\\nCROSS JOIN mortality_rate mr\\nCROSS JOIN medications meds;\",\n",
       " 'results':             avg_icu_length          mortality_rate hcpcs_cd\n",
       " 0     340.7963888888888889  0.47578258838187198256    G0378\n",
       " 1     340.7963888888888889  0.47578258838187198256    99219\n",
       " 2     340.7963888888888889  0.47578258838187198256    99218\n",
       " 3     340.7963888888888889  0.47578258838187198256    99220\n",
       " 4     340.7963888888888889  0.47578258838187198256    43264\n",
       " ...                    ...                     ...      ...\n",
       " 9995   21.2352777777777778  0.47578258838187198256    G0378\n",
       " 9996   21.2352777777777778  0.47578258838187198256    99219\n",
       " 9997   21.2352777777777778  0.47578258838187198256    99218\n",
       " 9998   21.2352777777777778  0.47578258838187198256    99220\n",
       " 9999   21.2352777777777778  0.47578258838187198256    43264\n",
       " \n",
       " [10000 rows x 3 columns],\n",
       " 'answer': \"Based on the query performed, for patients diagnosed with sepsis (ICD code 99591 or A41.9), the average length of stay in the Intensive Care Unit (ICU) is roughly 341 hours, which is approximately 14 days. \\n\\nThe mortality rate for these patients is approximately 47.58%. This implies that nearly half of the patients diagnosed with sepsis do not survive, emphasizing the seriousness of this condition.\\n\\nThe five most commonly administered medications, based on their HCPCS codes, are G0378, 99219, 99218, 99220, and 43264.\\n\\nThe HCPCS codes represent specific services or procedures. However, the results do not provide the actual names or descriptions of these medications. To get a clearer picture, you might consider querying the 'd_hcpcs' table with these codes to fetch the names and descriptions of these medications. \\n\\nFinally, to deepen the analysis, it could be interesting to study how the average length of ICU stay or the mortality rate changes based on different factors like age, gender, or comorbidities. Additionally, studying patterns or changes over time in medication administration could be insightful, possibly revealing changes in treatment strategy or the introduction and effectiveness of new therapies.\"}"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_complex_medical_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
