{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API configuration\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# PostgreSQL database connection\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5433\"  # Ensure this matches your PostgreSQL container port\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"Create and return database connection and cursor\"\"\"\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    cur = conn.cursor()\n",
    "    return conn, cur\n",
    "\n",
    "def fetch_all_metadata():\n",
    "    \"\"\"Fetch metadata for all tables\"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        # Get metadata for all tables\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_name, description, table_purpose, columns_info, \n",
    "                   primary_keys, foreign_keys, important_considerations,\n",
    "                   common_joins, example_questions\n",
    "            FROM mimic_table_metadata;\n",
    "        \"\"\")\n",
    "        \n",
    "        all_metadata = cur.fetchall()\n",
    "        \n",
    "        # Format metadata as dictionary\n",
    "        tables_metadata = {}\n",
    "        for row in all_metadata:\n",
    "            table_name = row[0]\n",
    "            tables_metadata[table_name] = {\n",
    "                'description': row[1],\n",
    "                'table_purpose': row[2],\n",
    "                'columns_info': row[3],\n",
    "                'primary_keys': row[4],\n",
    "                'foreign_keys': row[5],\n",
    "                'important_considerations': row[6],\n",
    "                'common_joins': row[7],\n",
    "                'example_questions': row[8]\n",
    "            }\n",
    "        \n",
    "        return tables_metadata\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def format_metadata_for_prompt(metadata):\n",
    "    \"\"\"Format metadata into text suitable for LLM prompt\"\"\"\n",
    "    formatted_text = \"# Database Schema Information\\n\\n\"\n",
    "    \n",
    "    for table_name, table_info in metadata.items():\n",
    "        formatted_text += f\"## Table: {table_name}\\n\"\n",
    "        formatted_text += f\"Description: {table_info['description']}\\n\"\n",
    "        formatted_text += f\"Purpose: {table_info['table_purpose']}\\n\\n\"\n",
    "        \n",
    "        # Add primary key information\n",
    "        if table_info['primary_keys']:\n",
    "            formatted_text += f\"Primary Keys: {', '.join(table_info['primary_keys'])}\\n\\n\"\n",
    "        \n",
    "        # Add foreign key information\n",
    "        if table_info['foreign_keys']:\n",
    "            formatted_text += \"Foreign Keys:\\n\"\n",
    "            for fk_col, fk_info in table_info['foreign_keys'].items():\n",
    "                formatted_text += f\"- {fk_col} references {fk_info['table']}.{fk_info['column']}\\n\"\n",
    "            formatted_text += \"\\n\"\n",
    "        \n",
    "        # Add column information\n",
    "        formatted_text += \"Columns:\\n\"\n",
    "        for col_name, col_info in table_info['columns_info'].items():\n",
    "            formatted_text += f\"- {col_name} ({col_info['data_type']}): {col_info['description']}\\n\"\n",
    "            \n",
    "            # Add categorical value distribution if available and not too long\n",
    "            if 'categorical_values' in col_info and len(col_info['categorical_values']) < 15:\n",
    "                formatted_text += f\"  Possible values: {', '.join(col_info['categorical_values'])}\\n\"\n",
    "            \n",
    "            # Add value range if available\n",
    "            if 'value_range' in col_info:\n",
    "                formatted_text += f\"  Range: {col_info['value_range']['min']} to {col_info['value_range']['max']}\\n\"\n",
    "        \n",
    "        formatted_text += \"\\n\"\n",
    "        \n",
    "        # Add important considerations\n",
    "        if table_info['important_considerations']:\n",
    "            formatted_text += f\"Important Considerations: {table_info['important_considerations']}\\n\\n\"\n",
    "        \n",
    "        # Add common joins\n",
    "        if table_info['common_joins']:\n",
    "            formatted_text += \"Common Joins:\\n\"\n",
    "            for join in table_info['common_joins']:\n",
    "                formatted_text += f\"- {join}\\n\"\n",
    "            formatted_text += \"\\n\"\n",
    "        \n",
    "        formatted_text += \"---\\n\\n\"\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_relevant_metadata(query_embedding, top_k=10):\n",
    "    \"\"\"\n",
    "    Fetch metadata for tables most relevant to the query embedding using Python-based similarity\n",
    "    \n",
    "    Parameters:\n",
    "        query_embedding (list): Vector representation of the user query\n",
    "        top_k (int): Number of most relevant tables to return\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping table names to their metadata\n",
    "    \"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Computing similarity to find top {top_k} relevant tables\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                table_name, description, table_purpose, columns_info, \n",
    "                primary_keys, foreign_keys, important_considerations,\n",
    "                common_joins, example_questions, embedding\n",
    "            FROM mimic_table_metadata\n",
    "            WHERE embedding IS NOT NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        rows = cur.fetchall()\n",
    "        \n",
    "        # Calculate similarity for each table\n",
    "        table_similarities = []\n",
    "        for row in rows:\n",
    "            table_name = row[0]\n",
    "            table_embedding = row[9]\n",
    "            \n",
    "            # Skip if embedding is NULL\n",
    "            if table_embedding is None:\n",
    "                print(f\"Skipping table {table_name} as embedding is NULL\")\n",
    "                continue\n",
    "            \n",
    "            # Debug information\n",
    "            print(f\"Table: {table_name}\")\n",
    "            print(f\"Embedding type: {type(table_embedding)}\")\n",
    "            print(f\"Embedding first few elements: {str(table_embedding)[:100]}...\")\n",
    "            \n",
    "            # Convert embedding to numeric list if needed\n",
    "            try:\n",
    "                # If embedding is stored as JSON string\n",
    "                if isinstance(table_embedding, str):\n",
    "                    import json\n",
    "                    table_embedding = json.loads(table_embedding)\n",
    "                # If embedding is stored as Python dict/list directly\n",
    "                elif isinstance(table_embedding, dict) or hasattr(table_embedding, 'keys'):\n",
    "                    # For JSONB stored as Python dict\n",
    "                    table_embedding = list(table_embedding.values())\n",
    "                \n",
    "                # Ensure numeric array\n",
    "                table_embedding = [float(x) for x in table_embedding]\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = cosine_similarity(query_embedding, table_embedding)\n",
    "                table_similarities.append((row, similarity))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing embedding for table {table_name}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        # Sort by similarity (descending) and take top_k\n",
    "        table_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_tables = table_similarities[:top_k]\n",
    "        \n",
    "        # Format as dictionary\n",
    "        tables_metadata = {}\n",
    "        for row, similarity in top_tables:\n",
    "            table_name = row[0]\n",
    "            tables_metadata[table_name] = {\n",
    "                'description': row[1],\n",
    "                'table_purpose': row[2],\n",
    "                'columns_info': row[3],\n",
    "                'primary_keys': row[4],\n",
    "                'foreign_keys': row[5],\n",
    "                'important_considerations': row[6],\n",
    "                'common_joins': row[7],\n",
    "                'example_questions': row[8]\n",
    "            }\n",
    "            \n",
    "            # Print similarity for debugging\n",
    "            print(f\"Table: {table_name}, Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        return tables_metadata\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors\n",
    "    \n",
    "    Parameters:\n",
    "        vec1 (list): First vector\n",
    "        vec2 (list): Second vector\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity (between -1 and 1)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompt(user_question, metadata_text):\n",
    "    \"\"\"Create the complete prompt to send to the LLM\"\"\"\n",
    "    prompt = f\"\"\"You are a professional medical database expert specializing in SQL and the MIMIC-IV database. Based on the user's question and the provided database metadata, generate a PostgreSQL query.\n",
    "\n",
    "## User Question\n",
    "{user_question}\n",
    "\n",
    "## Database Metadata\n",
    "{metadata_text}\n",
    "\n",
    "## Task\n",
    "1. Analyze the user question to determine which tables and columns need to be queried\n",
    "2. Design an effective SQL query based on the provided metadata\n",
    "3. Ensure the generated SQL is syntactically correct and considers table relationships\n",
    "4. If multiple table joins are needed, use the correct join conditions\n",
    "5. Handle any potential edge cases\n",
    "\n",
    "## Response Format\n",
    "Please return ONLY the SQL query without any explanation. Start your answer with \"SELECT\".\n",
    "\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_with_openai(prompt):\n",
    "    \"\"\"Generate SQL query using OpenAI API (updated for version 1.0+)\"\"\"\n",
    "    try:\n",
    "        # Using the new client format\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # or another suitable model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical database expert who converts natural language questions into PostgreSQL queries.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,  # Low temperature for more deterministic output\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Updated way to access response content\n",
    "        sql_query = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Remove \"SQL Query:\" prefix if present\n",
    "        if sql_query.startswith(\"SQL Query:\"):\n",
    "            sql_query = sql_query[len(\"SQL Query:\"):].strip()\n",
    "        \n",
    "        return sql_query\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def execute_sql_query(sql_query):\n",
    "    \"\"\"Execute SQL query and return results\"\"\"\n",
    "    conn, cur = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        cur.execute(sql_query)\n",
    "        \n",
    "        # Get column names\n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "        \n",
    "        # Get query results\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        df = pd.DataFrame(results, columns=column_names)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_user_query(query_text):\n",
    "    \"\"\"\n",
    "    Convert a user's natural language query into a vector representation\n",
    "    \n",
    "    Parameters:\n",
    "        query_text (str): The user's natural language query\n",
    "        \n",
    "    Returns:\n",
    "        list: Vector representation of the query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the query text - more processing steps can be added as needed\n",
    "        processed_query = query_text.strip()\n",
    "        \n",
    "        # Generate embedding vector using OpenAI API\n",
    "        response = client.embeddings.create(\n",
    "            input=processed_query,\n",
    "            model=\"text-embedding-3-small\"  # Use the same model as for table embeddings\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding vector\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        print(f\"✅ Successfully vectorized query: '{query_text[:50]}...' if len(query_text) > 50 else query_text\")\n",
    "        return query_embedding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error vectorizing query: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlrag_pipeline(user_question):\n",
    "    \"\"\"Execute the complete SQLRAG pipeline\"\"\"\n",
    "    print(f\"User Question: {user_question}\")\n",
    "    \n",
    "    # 1. Vectorize user query\n",
    "    print(\"Vectorizing user query...\")\n",
    "    query_embedding = vectorize_user_query(user_question)\n",
    "    \n",
    "    if not query_embedding:\n",
    "        return \"Failed to vectorize user query\"\n",
    "    \n",
    "    # 2. Fetch relevant metadata using vector similarity\n",
    "    print(\"Finding relevant tables...\")\n",
    "    metadata = fetch_relevant_metadata(query_embedding, top_k=5)\n",
    "    \n",
    "    # 3. Format metadata for prompt\n",
    "    metadata_text = format_metadata_for_prompt(metadata)\n",
    "    \n",
    "    # 4. Create LLM prompt\n",
    "    prompt = create_llm_prompt(user_question, metadata_text)\n",
    "    \n",
    "    print(\"Generating SQL with LLM...\")\n",
    "    # 5. Generate SQL query\n",
    "    sql_query = generate_sql_with_openai(prompt)\n",
    "    \n",
    "    if not sql_query:\n",
    "        return \"Failed to generate SQL query\"\n",
    "    \n",
    "    print(f\"Generated SQL: \\n{sql_query}\\n\")\n",
    "    \n",
    "    print(\"Executing SQL query...\")\n",
    "    # 6. Execute SQL query\n",
    "    results = execute_sql_query(sql_query)\n",
    "    \n",
    "    if results is None:\n",
    "        return \"Error executing SQL query\"\n",
    "    \n",
    "    print(\"Query Results:\")\n",
    "    print(results)\n",
    "    \n",
    "    return {\n",
    "        \"user_question\": user_question,\n",
    "        \"generated_sql\": sql_query,\n",
    "        \"results\": results\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
